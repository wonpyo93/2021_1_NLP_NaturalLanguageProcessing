# wp

TOPIC: Overcoming Catastrophic Forgetting in Graph Neural Networks
URL: https://arxiv.org/pdf/2012.06002.pdf
간략 설명: 'Catastrophic Forgetting'을 방지하는 Topology-Aware Weight Preserving (TWP)를 쓰자!
즉, node-classification task를 여러개 sequence적으로 하다 보면, 새로운 task를 배울 때마다 예전 것을 forgetting하게 된다.
Topology-aware Weight Preserving
    Minimized Loss Preserving
        각 task 마다 생성되는 Loss의 gradient
    Topological Structure Preserving
        attention의 task마다 달라지는 gradient
이 둘의 'importance'를 구해서 최종 loss나 H'을 만들때 적용하는 방법