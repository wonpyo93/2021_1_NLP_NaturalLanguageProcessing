# wp

TOPIC: Graph Representation Learning via Hard and Channel-Wise Attention Networks
URL: https://dl.acm.org/doi/pdf/10.1145/3292500.3330897
간략 설명: "저자 성이 Gao라서 그런지 Graph Attention OPERATOR라고 부르면서 이걸 GAO 라고 부른다...
hGAO, cGAO를 소개하는데, 둘 다 computational cost를 줄이는데 집중한다. (Cora, Citeseer, Pubmed 등의 성능이 0.2, 0.5 정도 오른걸로 보아 성능 면으론 큰 차이가 없는 듯...)
hGAO는 hard GAO로, 그냥 GAO를 soft GAO로 칭한다.
soft GAO는 center node가 자신의 '모든' 이웃 노드를 참고해야하는데, hGAO는 top K개만 참고한다. 참고의 기준은 trainable projection vector 'p'가 있을 때, feature vector X와 p를 project한 값을 가지고 가장 높은 top k개를 산출하는 방식.
cGAO는 Channel-Wise GAO로, node대node-feature vector로 attention을 뽑는 다른 GAO와는 달리, Channel을 이용해서 attention을 뽑는다.
근데 cGAO의 각 channel을 Xi라고 부르는데 (pg 744), 이게 각 채널의 feature matrix끼리 곱하려는건지..?"