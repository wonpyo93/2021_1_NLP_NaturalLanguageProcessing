# wp 2021-04-07 까지

TOPIC: Robust Hierarchical Graph Classification with
Subgraph Attention
URL: https://arxiv.org/pdf/2007.10908.pdf
간략 설명: 이 논문의 Task = to predict the label of a graph (node의 label을 구하는게 아님)
'GNN은 노드끼리 비교를 하지만, real world에선 sub-structure에 따라 label이 사실상 결정된다'
  한 노드마다 subgraph들을 만들어내는데, 그 subgraph 안에 들어가는 총 노드 수는 T로 정한다. 이 때, subgraph의 수가 엄청 많아질 수 있는데, 그래서 L이라는 숫자를 만들고 subgraph 수가 L보다 많아지면 랜덤샘플링으로 subgraph를 선택한다. 만약 L이 더 크면, round robin sampling으로 subgraph 수를 뻥튀기한다.
  3.1.2 (여러가지 방법을 했는데 concatenation of features가 제일 좋다 '카더라')
여기서 concate 할려면 각 subgraph의 feature#가 같아야해서 작은 애들한테 000000을 붙여준다
T=4인데 3개짜리 subgraph면 (xi||xj||xk||0) 이렇게. 참고로 여기서 feature dimension = D (F 아님)
그다음 attention coefficient 알파il을 구함. i는 노드, l은 L 중 한 숫자. 즉, center node - sub graph 의 attention coefficient
이게 한 개의 subgraph attention layer에 대한 과정.
이 논문이 소개하는 과정은 사실 R 개의 level을 가진 architecture. 처음부터 시작해서 그래프 크기를 subgraph로 묶으면서 점점 더 줄이는 과정. 처음에는 SubGatt 임베딩 layer를 통하고 나머지는 다 GIN 임베딩 layer로 줄여나감. 
이렇게 줄여나가는데 있어서 noisy structure때문에 information loss가 일어날 수 있기 때문에 여기서 또 attention을 쓴다. 여기서 첫 layer는 빼고 두번째부터 r번째까지 level만 이용한다. 거기서 나오는 attention을 intra-level attention이라 하고, 각 level마다 나온걸 하나의 Xinter, (R-1 * K)로 만든 다음 마지막 attention 구함.
코멘트: 여태 읽어본 논문중에서 가장 깔끔하고 가장 이해하기 쉽고 가장 디테일한 설명까지 잘 잡아낸 논문이라고 생각한다.
(다 읽고나서 저자 확인해보니 역시 중국 사람이 아닌 인도사람이었다)