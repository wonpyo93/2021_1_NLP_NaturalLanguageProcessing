# Day 4. RNN & LSTM

We've learned the basics of Neural Networks, especially the FeedForward Neural Networks (NNLM). So far, we have only placed inputs and received outputs, one by one. However, what if we want to set the input sequentially? In other words, how do we put the **sequential data** as an input?

Let's think about what sequential data is for a second.

When you watch videos on Youtube or movies on Netflix, you'll be generating your own history of records. Have you noticed that Youtube and Netflix recommends you some new contents that interest you? Those come from your history. This history, the sequential data of your clicks or views, become the input of the model so that they can find new items that can be 'recommended'.

What happens if we ignore the fact that this data is 'sequential' and shove it into the FeedForward NNLM as an input?

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled.png)

http://introtodeeplearning.com/

Notice that in this Feedforward NNLM shown above, the inputs $x_1, x_2, ..., x_m$ doesn't really show any sequential difference in resulting $z_i$, because arrows from the inputs affect all hiddens $z$s. This means that even if we mix up the sequence of the data, we'll end up with the same results of hidden representations. 

Also, since Feedforward NNLMs have fixed size of inputs, $m$ number of input $x$s, we can't really place 'sequential' data, because the word 'sequential' implies that the inputs can be modified by adding new input values.

Therefore, 'Recurrent Neural Networks' (RNN) is introduced. 

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%201.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%201.png)

[https://wikidocs.net/22886](https://wikidocs.net/22886)

Compared to the Feedforward NNLM where we only had one sequence of '$x$ —> $z$ —> $y$', we have multiples of the sequences. In this figure above, with the term t (time), we have t number of the sequences. 

Going back to Feedforward NNLM, when we create $z$ from $x$, we just summed all of $x_jw_{j,i}$ since we had a lot of $x$s. However, for RNN we only consider one $x$ each to 'update' $z$.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%202.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%202.png)

[http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

In this equation, note that $h_t$ refers to $z$ from Feedforward NNLM.

As mentioned, the hidden state $h_t$ is 'updated' from the previous hidden state $h_{t-1}$ and the new input $x_t$. This continues until all $x$s are have been inserted to finally get the last hidden state, thus resulting last output.

Well, from Feedforward NNLM, we had one parameter $W^{(1)}$, and since it was one-time performance, we didn't have to worry. What about RNN? Do we need many $W$s?

No, actually, $W$ in RNN is re-used. Let's look at a more detailed equation of $h_t$.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%203.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%203.png)

http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf

$f_W$is usually tanh, the parameter W is multiplied by the previous hidden state and new input. There are some pros and cons of using tanh, instead of some other functions like sigmoid. Sigmoid has its pros on the fact that it returns a value between 0 to 1, which means we can use the values as probability. However, it has a risk of vanishing gradient problem. Vanishing gradient problem in RNN is quite typical because as we go on and on, creating next-step $h$, it's inevitable to lose some old information while updating new ones. Tanh is better on this measure, as it has a slightly bigger gradient. Notice $hh$ and $xh$. These will not change by $t$, in other words, the parameter will not be affected by the 'sequence step' ($t$).

What are some applications of RNNs?

It depends on what kind of output we aim to get. 

If we aim many outputs, it would be 'Many-To-Many', and if we aim one single output, it would be 'Many-To-One'. These names literally make sense, because we have 'many' inputs, and we either aim 'One' or 'Many' outputs.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%204.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%204.png)

http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf

It could look like the figure above, where we get each output from each timestep $t$.

We could just use those $y$s, or we could combine them for further purposes.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%205.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%205.png)

http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf

Or, it could look like this figure above, where we have one output from updated $h_T$.

I mentioned above that by going through a lot of layers of RNNs, we could end up in a vanishing gradient problem. To alleviate this issue (I used alleviate because LSTM doesn't actually 'solve' the problem), LSTM has been proposed.

Long Short Term Memory (LSTM), simply put, is an RNN that has added a cell-state to $h$s.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%206.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%206.png)

In RNN, it was a straight-forward method to take every single information of given input to process, but in LSTM, we inject some adding or removing abilities in each cell. The figure above compares the typical RNN and LSTM, how each cell looks different.

We can see that in the yellow box, which is Neural Network Layer, we can see there are tanh and sigmoid (mentioned above). Each cell computes what kind of information should be removed or kept or even enhanced, to pass the updated $h$ to the next.

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%207.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%207.png)

The red circle, indicating pointwise operation, is just the addition and multiplication of matrices or vectors. $f$ indicates the forget gate, and $i$ indicates the input gate. $C$ indicates the cell state. Compared to $h$, $C$ is like a temporary memory to indicate what should be forgetted or added. 

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%208.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%208.png)

![Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%209.png](Day%204%20RNN%20&%20LSTM%2014c72629a3504d31b5a69b4200374854/Untitled%209.png)

After we get $C$, we can finally get the next $h$ uinsg $C_t$ that is newly computed, and $o_t$, where $o$ comes from the $h_{t-1}$.